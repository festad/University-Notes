#+TITLE: Big data


* Definizioni

Creano problemi alle tecnologie tradizionali,
ovvero
- architetture centralizzate (per superarle abbiamo
  bisogno dei cluster)
- database relazionali (si scontrano con le prerogative
  dei big data).

Non parliamo soltanto di qualcosa di grande,
ma anche di velocita' di elaborazione, acquisizione,
trasformazione in qualcosa di utile (applicazioni
in near real-time).
Parliamo quindi di
- memorizzazione
- gestione
- processing.

La seconda definizione sposta l'attenzione sui volumi,
i big data sono dataset le cui dimensioni rendono inabili
i tipici database (relazionali) per la loro gestione,
analisi.

La terza espande il focus e dice che i big data sono dataset
grandi e complessi.

Da queste definizioni possiamo dire che big data
non significa solo grosse dimensioni ma anche complessita',
velocita',
e non si tratta solo di gestione di basi di dati
ma anche una questione di potenza di calcolo ed elaborazione
(architettura centralizzata, il singolo nodo),
bisogna scalare orizzontalmente.
I big data creano problemi dall'acquisizione all'analisi.

Le ipotesi in machine learning sono che i dati arrivino con un formato speciale,
la fase iniziale e' quella infernale,
ovvero il data processing.

Cosa ha spinto i big data?
Forse le fonti operazionali?
Non proprio, i veri big data
arrivano da applicazioni basate su sensori (smart grid,
smart city) e social network (molto voluminosi e de-strutturati),
sono poco schematizzati ma devono comunque essere elaborati al volo.

L'utente non e' solo un consumatore di dati ma anche un produttore.

Le applicazioni per l'internet of things, la smart mobility,
l'agrifood, le smart cities, la telemedicina (spostare i pazienti
a casa loro).

Se ci chiedono quali sono le caratteristiche dei big data dobbiamo pensare
a volume, velocita' di elaborazione e immagazzinamento, variabilita'
dei dati.

Mantenere le performance all'aumentare del carico.

Scalabilita' verticale e scalabilita'
orizzontale (scale out, cluster, potenzialmente non
ha limiti).


elaborazione,
storage (filesystem distribuito),
database specificamente pensati (nosql),
scalabilita' orizzontale

Volume, velocita', varieta', variabilita' (valore = capacita' di modificare
immediatamente i dati in modo da renderli utilizzabili).

Limiti tecnologici che impongono.

Volume: zeptabyte, octabyte,
il volume impatta sul singolo nodo,
quando non ci stanno piu' si possono distribuire
su piu' nodi,
in realta' il volume e' influenzato anche da altri aspetti,
i big data arrivano in maniera molto disordinata
e sono dati che arrivano molto velocemente,
non c'e' il tempo di processarli, pulirli, elaborarli,
li dobbiamo parcheggiare.
Non ho li tempo di buttar via i dati inutili,
li prendo tutti e li parcheggio,
spesso potrei averne piu' di quel che serve,
ma a prescindere non posso sapere quali buttare,
questo ha impatto sui meccanismi di storage,
questo crea problemi con i meccanismi
dei database tradizionali.
Volume -> Problemi tipicamente sul singolo nodo.

Velocita':
i big data arrivano a grandi velocita' e devono essere processati
a grandi velocita',
se arrivano velocemente non ho il tempo di validarli
prima di salvarli o perderei altri dati (il buffer
si riempirebbe).
La velocita' impone di salvare i dati prima di usarli,
impatto soprattutto sui meccanismi di storage che
non possono piu' essere strutturati,
devono poter gestire dati non strutturati,
con ripetizioni, senza chiavi primarie..
Per essere veloce parallelizzo (architetture
distribuite),
anche perche' quando si parla di big data
si parla di azioni real time.

Varieta': aspetto piu' rognoso,
i dati non sono tutti strutturati,
non hanno vincoli o regole precise,
no vincoli di chiave.
La varieta' ha soprattutto impatto sui meccanismi di storage.


Schema on-write (devo avere
in mente lo schema mentre lo scrivo), schema on-read (stabilisco lo
schema quando lo leggo dal meccanismo di storage),
una tabella e' un insieme omogenero di record.
Union e intersection sono costruitti mysql, concetto di insieme.
piu' ad alto livello ci sono i concetti di lista.
I semistrutturati vengono scanditi cosi' come sono, se voglio
fare piu' alla svelta e' importante l'ordine con cui appaiono
in un documento semistrutturato, nei semistrutturati l'ordine
non e' indifferente.
Nello strutturato c'e' il concetto di normalizzazione,
nel semistrutturato c'e' il concetto di annidamento.

Nel mondo semistrutturato si tende a mettere tutto insieme
per velocizzare le cose piuttosto che usare tabelle separate,
il contro e' la ridondanza.

La differenza tra mondo strutturato e non, a parte lo schema on-write.. e non schema
e' che nel caso strutturato abbiamo sql (query sui dati),
nell'altro andiamo per keyword, si estraggono features dai video e sono
usate per fare ricerche.
Precision: #item_corretti/#item_prelevati,
perfetta quando non faccio niente,
se non restituisco niente ho recall zero.


#risultati_rilevanti/#risultati_rilevanti_nel_database -> recall,
quanto copro tutto cio' che stavo cercando,

All'interno di un db strutturato posso fare degli aggiornamenti,
l'update c'e' nel relazionale,
nel destrutturato no, piuttosto ho una nuova versione
che sostituisco alla precedente.


I big data hanno una qualita' scarsa,
problematiche di incompletezza,
non sempre sono consistenti,
non garantisco assenza di duplicazione,
non posso garantire che i vincoli di integrita'
siano rispettati.
Gli user-generated-content

- architetture distribuite, vedremo mapreduce, apache sparks, adupe,
- database nosql, mongodb (documentale), influxdb.

** Hadoop e DHFS

Sviluppo di architetture distribuite e problema
dello storage sono gli effetti dei big-data,
quando ho tanti dati da elaborare velocemente penso prima di tutto
a potenza di calcolo e spazio di memorizzazione,
sulle architetture distribuite ho bisogno di nuove
tecnologie.

Oggi parliamo di architetture distribuite.
Le distribuite permettono la scalabilita' orizzontale (mantenere
performance all'aunentare del carico),
posso poi garantire la scalabilita' lavorando orizzontalmente
(no limiti potenzialmente),
i cluster permettono maggiore resistenza ai guasti (fault tolerance),
l'eventual consistency e' la consistenza del sistema a regime,
con i dati replicati quando ho una modifica su un nodo questa
modifica si deve replicare sui nodi nella rete,
richiede un minimo di tempo,
in quel lasso non ho la consistenza,
le architetture distribuite moderne si basano
sul "shared nothing architecture",
che non si realizza sulle architetture verticali,
ci sono colli di bottiglia che sono le risorse condivise,
su una singola macchina c'e' qualcosa di condiviso che potrebbe
diventare il collo di bottiglia,
potrebbe rallentare,
mentre le distribuite non condividono questi
colli di bottiglia,
si parla di commodity hardware,
server che garantiscono la scalabilita' perche' ne metto
insieme tanti.
I singoli nodi hanno la loro memoria e i loro bus, interagiscono
il minimo possibile con gli altri.
Per sfruttare la shared nothing architecture anche
il paradigma di programmazione distribuita ne deve
tenere conto, come deve tenere conto
della Data Locality,
ovvero i dati posizionati al di sopra di un nodo
non devono spostarsi da quel nodo, perche'
vorrebbe dire intasare la rete.
I programmi tendono non a spostare il dato ma la funzionalita'
(paradigma MapReduce),
Teorema del CAP:
pensato per i distribuiti,
dice che un qualsiasi software distribuito non puo'
garantire contemporaneamente:
- consistency, capacita' di mostrare lo stesso dato su nodi diversi, (coerenza)
- availability, capacita' di rispondere alle richieste da parte del sistema,
- partition tolerance.

*** Consistenza
Per garantirla o rinuncio a availability o a partition tolerance,
per es se il nodo deve aggiornarsi, mentre si aggiorna non puo'
rispondere di quel dato in quel preciso momento.
Se voglio l'availability rinuncio alla coerenza.
Se invece rinuncio alla partition tolerance posso avere le altre due contemporaneamente
usando un solo nodo.

I relazionali sono pensati piu' per i centralizzati, sono
piu' acidi, mentre gli altri nosequel sono piu' basici,

Hadoop e' una suite di moduli pensata
per esecuzione di software parallelizzati su un'architettura
distribuita,
e' stato iniziato da Google ed e' ora in mano all'Apache Foundation,
Hadoop ha un filesystem distribuito
che hadoop virtualizza,
hdfs e' hadoop distributed file system,
Hadoop lavora ancora molto sul disco fisos, non e' imparato per
essere veloce.

Distributed filesystem:
funziona come un fs tradizionale, i file vengono
spezzati e distribuiti sui nodi,
il blocco minimo su cui viene spezzato il file
e' il chunk e non supera i 64/128 M,
il fattore di replica minimo e' 3, (dato replicato
3 volte),
il fs sotto ha due tipi di nodi:
- data node, contiene i dati
- master node, contiene il registro in cui sono scritte le info
  su dove sstanno i chunk e come leggere i dati sui data node,

il data note dice su quali nodi scrivere i file chunks e si segna
questa informazione,
quando leggo un file non devo sapere su quali nodi
cercare i ifle ma basta chiedere al master node,
il master node fa apparire il filesystem come
una serie di cartelle e di file.

L'applicazione comunica con il HDFS client,
gli chiede di scrivere o leggere file,
questa richiesta viene letta dal client che interagisce con
i nodes per sapere dove trovare questi file,
il client hdfs chiede al name node dove trovare il file
perche' il name node ha la tabella,
sa che il file e' spezzato in determinati chunks
e sa dove i chunks risiedono,
il name node aspetta un ack per evitare che il sistema
possa finire in uno stato inconsistente,
i data node rispondono con un ack al name node,
la scrittura non e' a stella ma in sequenza,

